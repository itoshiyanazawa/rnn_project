{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itoshiyanazawa/rnn_project/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "jdpJpXwdTlJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "JpVe6Z9ETs-6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1"
      ],
      "metadata": {
        "id": "0agUriRGTlkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Collect the text dataset:\n",
        "  *   Download the text dataset from a reputable source or use a pre-existing one.\n",
        "\n",
        "2. Clean the text data:\n",
        "  *   Remove unwanted characters, punctuation, and formatting.\n",
        "  *  Convert all text to lowercase to reduce complexity.\n",
        "\n",
        "3. Tokenize the text:\n",
        "  *   Split the text into individual characters.\n",
        "  *   Create a vocabulary of unique tokens and map each token to an integer.\n",
        "\n",
        "4. Create input sequences:\n",
        "  *   Generate input sequences and corresponding targets for training.\n",
        "\n"
      ],
      "metadata": {
        "id": "326JLY1eUDEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting the Movie Dialogues dataset from the Cornell University Website"
      ],
      "metadata": {
        "id": "dinGGf8mVE0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\"\n",
        "file_name = \"cornell_movie_dialogs_corpus.zip\"\n",
        "\n",
        "# Download the dataset\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "# Extract the dataset\n",
        "with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"cornell_data\")\n",
        "\n",
        "print(\"✅ Dataset downloaded and extracted.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVyLIsEVTvSa",
        "outputId": "6e8b2666-0cfd-4e58-8257-9fa265fba66e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset downloaded and extracted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning the Text Data"
      ],
      "metadata": {
        "id": "fpUheOSSVBGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dialogue lines\n",
        "with open(\"cornell_data/cornell movie-dialogs corpus/movie_lines.txt\", encoding='iso-8859-1') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Extract the actual text (5th field) from each line\n",
        "dialogues = []\n",
        "for line in lines:\n",
        "    parts = line.split(\" +++$+++ \")\n",
        "    if len(parts) == 5:\n",
        "        dialogues.append(parts[-1].strip())\n",
        "\n",
        "# Clean the text\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # lowercase\n",
        "    text = re.sub(r\"[^a-zA-Z0-9.,!?'\\n ]+\", ' ', text)  # remove special chars\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # normalize spacing\n",
        "    return text\n",
        "\n",
        "cleaned_dialogues = [clean_text(line) for line in dialogues]\n",
        "\n",
        "# Combine into one long string\n",
        "full_text = ' '.join(cleaned_dialogues)\n",
        "\n",
        "print(\"🧹 Cleaned text preview:\\n\", full_text[:500])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW4UFF4KU-Wi",
        "outputId": "0fd5a5e1-5a74-44a4-93c1-6c2b5c2fa6c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 Cleaned text preview:\n",
            " they do not! they do to! i hope so. she okay? let's go. wow okay you're gonna need to learn how to lie. no i'm kidding. you know how sometimes you just become this persona ? and you don't know how to quit? like my fear of wearing pastels? the real you . what good stuff? i figured you'd get to the good stuff eventually. thank god! if i had to hear one more story about your coiffure... me. this endless ...blonde babble. i'm like, boring myself. what crap? do you listen to this crap? no... then gui\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize the text"
      ],
      "metadata": {
        "id": "EDhAGZojVq6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Character-level tokenization\n",
        "chars = sorted(set(full_text))  # unique characters\n",
        "char2idx = {ch: idx for idx, ch in enumerate(chars)}  # map char to index\n",
        "idx2char = {idx: ch for ch, idx in char2idx.items()}  # map index to char\n",
        "\n",
        "# Convert all text to a sequence of integers\n",
        "text_as_int = [char2idx[c] for c in full_text]\n",
        "\n",
        "print(\"🧠 Total characters:\", len(full_text))\n",
        "print(\"🔤 Vocabulary size:\", len(chars))\n",
        "print(\"Sample encoding:\", text_as_int[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqrcrOjUVtr_",
        "outputId": "2524fdc7-759f-4434-c71b-558e1f3d2519"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Total characters: 16940151\n",
            "🔤 Vocabulary size: 42\n",
            "Sample encoding: [35, 23, 20, 40, 0, 19, 30, 0, 29, 30, 35, 1, 0, 35, 23, 20, 40, 0, 19, 30]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate input sequences and corresponding targets for training"
      ],
      "metadata": {
        "id": "AV8TXH4NV-GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sequence length\n",
        "seq_length = 100\n",
        "\n",
        "# Convert list to NumPy array for faster slicing\n",
        "text_as_int = np.array(text_as_int)\n",
        "\n",
        "# Calculate number of sequences\n",
        "num_sequences = len(text_as_int) - seq_length\n",
        "\n",
        "# Create input sequences and targets using vectorized slicing\n",
        "sequences = np.array([text_as_int[i:i+seq_length] for i in range(num_sequences)])\n",
        "targets = text_as_int[seq_length:]  # targets are simply shifted by one position\n",
        "\n",
        "print(\"✅ Total sequences created:\", len(sequences))\n",
        "print(\"Sample input:\", sequences[0])\n",
        "print(\"Sample target:\", targets[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OndtXT6V_Tw",
        "outputId": "55099a4a-4c10-4571-9595-046dd79416e4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Total sequences created: 16940051\n",
            "Sample input: [35 23 20 40  0 19 30  0 29 30 35  1  0 35 23 20 40  0 19 30  0 35 30  1\n",
            "  0 24  0 23 30 31 20  0 34 30  4  0 34 23 20  0 30 26 16 40 15  0 27 20\n",
            " 35  2 34  0 22 30  4  0 38 30 38  0 30 26 16 40  0 40 30 36  2 33 20  0\n",
            " 22 30 29 29 16  0 29 20 20 19  0 35 30  0 27 20 16 33 29  0 23 30 38  0\n",
            " 35 30  0 27]\n",
            "Sample target: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2"
      ],
      "metadata": {
        "id": "z25LiasFTvqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Define the RNN architecture (e.g. using Tensorflow or PyTorch).\n",
        "2. Explain the type of layers you are including and why (layers such as Embedding, LSTM, and Linear)\n",
        "3. Visualize your RNN architecture\n",
        "4. Compile the model with appropriate loss function and optimizer. Explain your choice of loos function and optimizer.\n",
        "5. Prepare data for training by converting sequences and targets into batches.\n",
        "6. Train the model on the training data and validate it on the validation set.\n",
        "7. Visualize the training process using both training and validation results.\n"
      ],
      "metadata": {
        "id": "xuHtPOF2i57S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the RNN Architecture"
      ],
      "metadata": {
        "id": "3j4wq9cZjnkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(char2idx)\n",
        "\n",
        "# Define a deeper model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=128),  # Increased embedding size\n",
        "    LSTM(256, return_sequences=True),\n",
        "    LSTM(256),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "RSGRDzyaTyoT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Layer Breakdown**:\n",
        "- `Embedding`: Converts integer indices into dense 64-dimensional vectors. This helps the model learn semantic relationships between characters.\n",
        "- `LSTM`: Long Short-Term Memory layer with 128 units, captures temporal dependencies and handles vanishing gradient issues better than simple RNNs.\n",
        "- `Dense`: Fully connected output layer with `softmax` activation, outputs probabilities across the vocabulary to predict the next character.\n"
      ],
      "metadata": {
        "id": "YLVpdZWAmNlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the Model"
      ],
      "metadata": {
        "id": "ZObyTrUCnYx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "IVXDHhpdmVSD",
        "outputId": "8f3cac02-a348-403a-b7a3-07b5f1b3741c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compiling the model with appropriate loss function and optimizer.\n",
        "**Loss Function**:\n",
        "- We use `sparse_categorical_crossentropy` because the target output is a single integer (not one-hot encoded).\n",
        "\n",
        "**Optimizer**:\n",
        "- `Adam` is used for its adaptive learning rate, helping the model converge faster and more reliably during training.\n"
      ],
      "metadata": {
        "id": "K6cnqCUGnbb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "-rUUOQnrnn4V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare data for training"
      ],
      "metadata": {
        "id": "xPE4L3xjnxiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Reduce data if necessary (optional sampling)\n",
        "max_examples = 120000  # You can tune this down if needed\n",
        "sequences = sequences[:max_examples]\n",
        "targets = targets[:max_examples]\n",
        "\n",
        "# Step 2: Convert to tf.data.Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((sequences, targets))\n",
        "\n",
        "# Step 3: Shuffle and split into train/validation\n",
        "buffer_size = len(sequences)\n",
        "batch_size = 64\n",
        "\n",
        "# Shuffle and batch\n",
        "train_size = int(0.9 * buffer_size)\n",
        "val_size = buffer_size - train_size\n",
        "\n",
        "train_dataset = dataset.take(train_size).shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "val_dataset = dataset.skip(train_size).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "print(\"✅ Train batches:\", len(train_dataset))\n",
        "print(\"✅ Validation batches:\", len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFGR6344nyVa",
        "outputId": "67dd6d6f-6cb7-4111-9b4e-6c003cc944fd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Train batches: 1687\n",
            "✅ Validation batches: 187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Model"
      ],
      "metadata": {
        "id": "pkZ1pxxVqlmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=val_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "btQOItnDqknN",
        "outputId": "ecad177b-cd04-43d5-c472-4692cd5782f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m1627/1687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1:02\u001b[0m 1s/step - accuracy: 0.2910 - loss: 2.5318"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing Model Accuracy and Loss"
      ],
      "metadata": {
        "id": "tNHP0OzDqpgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "\n",
        "# Accuracy plot\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_dict['accuracy'], label='Training Accuracy', marker='o')\n",
        "plt.plot(history_dict['val_accuracy'], label='Validation Accuracy', marker='s')\n",
        "plt.title('Model Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_dict['loss'], label='Training Loss', marker='o')\n",
        "plt.plot(history_dict['val_loss'], label='Validation Loss', marker='s')\n",
        "plt.title('Model Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XMwCjBpSqxgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3"
      ],
      "metadata": {
        "id": "z1gcJ_mRTzj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Create a function to generate text by sampling from the model's predictions.\n",
        "\n",
        "Qualitative Evaluation:\n",
        "\n",
        "2. Coherence and Grammar: Check if the generated text is grammatically correct and coherent.\n",
        "\n",
        "3. Creativity: Evaluate if the generated text is creative and interesting.\n",
        "\n",
        "4. Contextual Relevance: Assess whether the generated text maintains context and follows logically from the seed text.\n",
        "\n",
        "5. Diversity: Ensure that the model does not repeat itself excessively and generates diverse outputs.\n",
        "\n",
        "Quantitative Evaluation:\n",
        "\n",
        "6. Perplexity: A common metric for evaluating language models. Perplexity measures how well the model predicts the next token in a sequence. Lower perplexity indicates better performance.\n",
        "\n",
        "7. BLEU Score: Used to evaluate the quality of text that has been machine-translated from one language to another. It can be adapted to measure the overlap between generated text and reference text.\n",
        "\n",
        "8. ROUGE Score: Commonly used for evaluating summarization and translation models. It measures the overlap of n-grams between the generated text and a reference text.\n",
        "\n",
        "9. Entropy and Repetition Metrics: Measure the diversity of the generated text. High entropy and low repetition indicate diverse and less repetitive outputs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EKEPJXYFU1qB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, num_generate=300, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generates text using the trained model.\n",
        "    \"\"\"\n",
        "    start_string = start_string.lower()\n",
        "\n",
        "    try:\n",
        "        input_indices = [char2idx[char] for char in start_string]\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: Character '{e.args[0]}' not in vocabulary.\")\n",
        "        return \"\"\n",
        "\n",
        "    input_indices = np.expand_dims(input_indices, axis=0)\n",
        "    generated_text = start_string\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model.predict(input_indices)\n",
        "        predictions = predictions[0]  # Shape: (vocab_size,)\n",
        "\n",
        "        # Apply temperature adjustment\n",
        "        predictions = predictions / temperature\n",
        "        predicted_idx = np.random.choice(len(chars), p=np.exp(predictions) / np.sum(np.exp(predictions)))\n",
        "\n",
        "        predicted_char = idx2char[predicted_idx]\n",
        "        generated_text += predicted_char\n",
        "\n",
        "        # Update input sequence\n",
        "        input_indices = np.append(input_indices[0][1:], predicted_idx)\n",
        "        input_indices = np.expand_dims(input_indices, axis=0)\n",
        "\n",
        "    return generated_text\n"
      ],
      "metadata": {
        "id": "kJr6ZNvCT1Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"Once upon a time, \"\n",
        "\n",
        "print(\"Temperature 0.2 (Predictable):\\n\")\n",
        "print(generate_text(model, start_string=seed_text, num_generate=300, temperature=0.2))\n",
        "\n",
        "print(\"\\nTemperature 0.5 (Balanced):\\n\")\n",
        "print(generate_text(model, start_string=seed_text, num_generate=300, temperature=0.5))\n",
        "\n",
        "print(\"\\nTemperature 1.0 (Creative):\\n\")\n",
        "print(generate_text(model, start_string=seed_text, num_generate=300, temperature=1.0))\n"
      ],
      "metadata": {
        "id": "iBQRzDyOTMAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4"
      ],
      "metadata": {
        "id": "daklMQk7i2n8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Experiment with different architectures (e.g., adding more layers, or trying other layer types).\n",
        " * Try deeper networks, different activation functions, or different layer configurations.\n",
        " * Example: Adding more LSTM layers or using GRU instead of LSTM.\n",
        "2. Apply regularization techniques (e.g., Use dropout to prevent overfitting).\n",
        "3. Use advanced text preprocessing.\n",
        " * Implement techniques like stemming, lemmatization, or BPE (Byte Pair Encoding) for\n",
        "better tokenization.\n",
        "4. Fine-tune hyperparameters (e.g., learning rate, batch size).\n",
        " * Experiment with different learning rates, batch sizes, and epochs.\n",
        " * Explain your approach for fine-tuning the hyper-parameters."
      ],
      "metadata": {
        "id": "NAmXTKKUVXWc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q58FY70Yi4Wn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}