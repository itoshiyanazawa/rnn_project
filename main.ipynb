{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itoshiyanazawa/rnn_project/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "jdpJpXwdTlJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "JpVe6Z9ETs-6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1"
      ],
      "metadata": {
        "id": "0agUriRGTlkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Collect the text dataset:\n",
        "  *   Download the text dataset from a reputable source or use a pre-existing one.\n",
        "\n",
        "2. Clean the text data:\n",
        "  *   Remove unwanted characters, punctuation, and formatting.\n",
        "  *  Convert all text to lowercase to reduce complexity.\n",
        "\n",
        "3. Tokenize the text:\n",
        "  *   Split the text into individual characters.\n",
        "  *   Create a vocabulary of unique tokens and map each token to an integer.\n",
        "\n",
        "4. Create input sequences:\n",
        "  *   Generate input sequences and corresponding targets for training.\n",
        "\n"
      ],
      "metadata": {
        "id": "326JLY1eUDEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting the Movie Dialogues dataset from the Cornell University Website"
      ],
      "metadata": {
        "id": "dinGGf8mVE0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\"\n",
        "file_name = \"cornell_movie_dialogs_corpus.zip\"\n",
        "\n",
        "# Download the dataset\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "# Extract the dataset\n",
        "with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"cornell_data\")\n",
        "\n",
        "print(\"✅ Dataset downloaded and extracted.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVyLIsEVTvSa",
        "outputId": "b73749b6-775d-4bf0-876a-4a48574e4569"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset downloaded and extracted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning the Text Data"
      ],
      "metadata": {
        "id": "fpUheOSSVBGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dialogue lines\n",
        "with open(\"cornell_data/cornell movie-dialogs corpus/movie_lines.txt\", encoding='iso-8859-1') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Extract the actual text (5th field) from each line\n",
        "dialogues = []\n",
        "for line in lines:\n",
        "    parts = line.split(\" +++$+++ \")\n",
        "    if len(parts) == 5:\n",
        "        dialogues.append(parts[-1].strip())\n",
        "\n",
        "# Clean the text\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # lowercase\n",
        "    text = re.sub(r\"[^a-zA-Z0-9.,!?'\\n ]+\", ' ', text)  # remove special chars\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # normalize spacing\n",
        "    return text\n",
        "\n",
        "cleaned_dialogues = [clean_text(line) for line in dialogues]\n",
        "\n",
        "# Combine into one long string\n",
        "full_text = ' '.join(cleaned_dialogues)\n",
        "\n",
        "print(\"🧹 Cleaned text preview:\\n\", full_text[:500])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW4UFF4KU-Wi",
        "outputId": "0639eafc-a61f-40e4-a001-8ed07cbcbf17"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 Cleaned text preview:\n",
            " they do not! they do to! i hope so. she okay? let's go. wow okay you're gonna need to learn how to lie. no i'm kidding. you know how sometimes you just become this persona ? and you don't know how to quit? like my fear of wearing pastels? the real you . what good stuff? i figured you'd get to the good stuff eventually. thank god! if i had to hear one more story about your coiffure... me. this endless ...blonde babble. i'm like, boring myself. what crap? do you listen to this crap? no... then gui\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize the text"
      ],
      "metadata": {
        "id": "EDhAGZojVq6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Character-level tokenization\n",
        "chars = sorted(set(full_text))  # unique characters\n",
        "char2idx = {ch: idx for idx, ch in enumerate(chars)}  # map char to index\n",
        "idx2char = {idx: ch for ch, idx in char2idx.items()}  # map index to char\n",
        "\n",
        "# Convert all text to a sequence of integers\n",
        "text_as_int = [char2idx[c] for c in full_text]\n",
        "\n",
        "print(\"🧠 Total characters:\", len(full_text))\n",
        "print(\"🔤 Vocabulary size:\", len(chars))\n",
        "print(\"Sample encoding:\", text_as_int[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqrcrOjUVtr_",
        "outputId": "2524fdc7-759f-4434-c71b-558e1f3d2519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Total characters: 16940151\n",
            "🔤 Vocabulary size: 42\n",
            "Sample encoding: [35, 23, 20, 40, 0, 19, 30, 0, 29, 30, 35, 1, 0, 35, 23, 20, 40, 0, 19, 30]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate input sequences and corresponding targets for training"
      ],
      "metadata": {
        "id": "AV8TXH4NV-GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sequence length\n",
        "seq_length = 100\n",
        "\n",
        "# Convert list to NumPy array for faster slicing\n",
        "text_as_int = np.array(text_as_int)\n",
        "\n",
        "# Calculate number of sequences\n",
        "num_sequences = len(text_as_int) - seq_length\n",
        "\n",
        "# Create input sequences and targets using vectorized slicing\n",
        "sequences = np.array([text_as_int[i:i+seq_length] for i in range(num_sequences)])\n",
        "targets = text_as_int[seq_length:]  # targets are simply shifted by one position\n",
        "\n",
        "print(\"✅ Total sequences created:\", len(sequences))\n",
        "print(\"Sample input:\", sequences[0])\n",
        "print(\"Sample target:\", targets[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OndtXT6V_Tw",
        "outputId": "55099a4a-4c10-4571-9595-046dd79416e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Total sequences created: 16940051\n",
            "Sample input: [35 23 20 40  0 19 30  0 29 30 35  1  0 35 23 20 40  0 19 30  0 35 30  1\n",
            "  0 24  0 23 30 31 20  0 34 30  4  0 34 23 20  0 30 26 16 40 15  0 27 20\n",
            " 35  2 34  0 22 30  4  0 38 30 38  0 30 26 16 40  0 40 30 36  2 33 20  0\n",
            " 22 30 29 29 16  0 29 20 20 19  0 35 30  0 27 20 16 33 29  0 23 30 38  0\n",
            " 35 30  0 27]\n",
            "Sample target: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2"
      ],
      "metadata": {
        "id": "z25LiasFTvqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Define the RNN architecture (e.g. using Tensorflow or PyTorch).\n",
        "2. Explain the type of layers you are including and why (layers such as Embedding, LSTM, and Linear)\n",
        "3. Visualize your RNN architecture\n",
        "4. Compile the model with appropriate loss function and optimizer. Explain your choice of loos function and optimizer.\n",
        "5. Prepare data for training by converting sequences and targets into batches.\n",
        "6. Train the model on the training data and validate it on the validation set.\n",
        "7. Visualize the training process using both training and validation results.\n"
      ],
      "metadata": {
        "id": "xuHtPOF2i57S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the RNN Architecture"
      ],
      "metadata": {
        "id": "3j4wq9cZjnkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(char2idx)\n",
        "\n",
        "# Define a deeper model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=128),  # Increased embedding size\n",
        "    LSTM(256, return_sequences=True),\n",
        "    LSTM(256),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "RSGRDzyaTyoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Layer Breakdown**:\n",
        "- `Embedding`: Converts integer indices into dense 64-dimensional vectors. This helps the model learn semantic relationships between characters.\n",
        "- `LSTM`: Long Short-Term Memory layer with 128 units, captures temporal dependencies and handles vanishing gradient issues better than simple RNNs.\n",
        "- `Dense`: Fully connected output layer with `softmax` activation, outputs probabilities across the vocabulary to predict the next character.\n"
      ],
      "metadata": {
        "id": "YLVpdZWAmNlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the Model"
      ],
      "metadata": {
        "id": "ZObyTrUCnYx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "IVXDHhpdmVSD",
        "outputId": "8f3cac02-a348-403a-b7a3-07b5f1b3741c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compiling the model with appropriate loss function and optimizer.\n",
        "**Loss Function**:\n",
        "- We use `sparse_categorical_crossentropy` because the target output is a single integer (not one-hot encoded).\n",
        "\n",
        "**Optimizer**:\n",
        "- `Adam` is used for its adaptive learning rate, helping the model converge faster and more reliably during training.\n"
      ],
      "metadata": {
        "id": "K6cnqCUGnbb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "-rUUOQnrnn4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare data for training"
      ],
      "metadata": {
        "id": "xPE4L3xjnxiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Reduce data if necessary (optional sampling)\n",
        "max_examples = 120000  # You can tune this down if needed\n",
        "sequences = sequences[:max_examples]\n",
        "targets = targets[:max_examples]\n",
        "\n",
        "# Step 2: Convert to tf.data.Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((sequences, targets))\n",
        "\n",
        "# Step 3: Shuffle and split into train/validation\n",
        "buffer_size = len(sequences)\n",
        "batch_size = 64\n",
        "\n",
        "# Shuffle and batch\n",
        "train_size = int(0.9 * buffer_size)\n",
        "val_size = buffer_size - train_size\n",
        "\n",
        "train_dataset = dataset.take(train_size).shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "val_dataset = dataset.skip(train_size).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "print(\"✅ Train batches:\", len(train_dataset))\n",
        "print(\"✅ Validation batches:\", len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFGR6344nyVa",
        "outputId": "67dd6d6f-6cb7-4111-9b4e-6c003cc944fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Train batches: 1687\n",
            "✅ Validation batches: 187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Model"
      ],
      "metadata": {
        "id": "pkZ1pxxVqlmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=val_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "btQOItnDqknN",
        "outputId": "ecad177b-cd04-43d5-c472-4692cd5782f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m1627/1687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1:02\u001b[0m 1s/step - accuracy: 0.2910 - loss: 2.5318"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing Model Accuracy and Loss"
      ],
      "metadata": {
        "id": "tNHP0OzDqpgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "\n",
        "# Accuracy plot\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_dict['accuracy'], label='Training Accuracy', marker='o')\n",
        "plt.plot(history_dict['val_accuracy'], label='Validation Accuracy', marker='s')\n",
        "plt.title('Model Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_dict['loss'], label='Training Loss', marker='o')\n",
        "plt.plot(history_dict['val_loss'], label='Validation Loss', marker='s')\n",
        "plt.title('Model Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XMwCjBpSqxgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3"
      ],
      "metadata": {
        "id": "z1gcJ_mRTzj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Create a function to generate text by sampling from the model's predictions.\n",
        "\n",
        "Qualitative Evaluation:\n",
        "\n",
        "2. Coherence and Grammar: Check if the generated text is grammatically correct and coherent.\n",
        "\n",
        "3. Creativity: Evaluate if the generated text is creative and interesting.\n",
        "\n",
        "4. Contextual Relevance: Assess whether the generated text maintains context and follows logically from the seed text.\n",
        "\n",
        "5. Diversity: Ensure that the model does not repeat itself excessively and generates diverse outputs.\n",
        "\n",
        "Quantitative Evaluation:\n",
        "\n",
        "6. Perplexity: A common metric for evaluating language models. Perplexity measures how well the model predicts the next token in a sequence. Lower perplexity indicates better performance.\n",
        "\n",
        "7. BLEU Score: Used to evaluate the quality of text that has been machine-translated from one language to another. It can be adapted to measure the overlap between generated text and reference text.\n",
        "\n",
        "8. ROUGE Score: Commonly used for evaluating summarization and translation models. It measures the overlap of n-grams between the generated text and a reference text.\n",
        "\n",
        "9. Entropy and Repetition Metrics: Measure the diversity of the generated text. High entropy and low repetition indicate diverse and less repetitive outputs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EKEPJXYFU1qB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, num_generate=300, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generates text using the trained model.\n",
        "    \"\"\"\n",
        "    start_string = start_string.lower()\n",
        "\n",
        "    try:\n",
        "        input_indices = [char2idx[char] for char in start_string]\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: Character '{e.args[0]}' not in vocabulary.\")\n",
        "        return \"\"\n",
        "\n",
        "    input_indices = np.expand_dims(input_indices, axis=0)\n",
        "    generated_text = start_string\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model.predict(input_indices)\n",
        "        predictions = predictions[0]  # Shape: (vocab_size,)\n",
        "\n",
        "        # Apply temperature adjustment\n",
        "        predictions = predictions / temperature\n",
        "        predicted_idx = np.random.choice(len(chars), p=np.exp(predictions) / np.sum(np.exp(predictions)))\n",
        "\n",
        "        predicted_char = idx2char[predicted_idx]\n",
        "        generated_text += predicted_char\n",
        "\n",
        "        # Update input sequence\n",
        "        input_indices = np.append(input_indices[0][1:], predicted_idx)\n",
        "        input_indices = np.expand_dims(input_indices, axis=0)\n",
        "\n",
        "    return generated_text\n"
      ],
      "metadata": {
        "id": "kJr6ZNvCT1Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"Once upon a time, \"\n",
        "\n",
        "print(\"Temperature 0.2 (Predictable):\\n\")\n",
        "print(generate_text(model, start_string=seed_text, num_generate=300, temperature=0.2))\n",
        "\n",
        "print(\"\\nTemperature 0.5 (Balanced):\\n\")\n",
        "print(generate_text(model, start_string=seed_text, num_generate=300, temperature=0.5))\n",
        "\n",
        "print(\"\\nTemperature 1.0 (Creative):\\n\")\n",
        "print(generate_text(model, start_string=seed_text, num_generate=300, temperature=1.0))\n"
      ],
      "metadata": {
        "id": "iBQRzDyOTMAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4"
      ],
      "metadata": {
        "id": "daklMQk7i2n8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Experiment with different architectures (e.g., adding more layers, or trying other layer types).\n",
        " * Try deeper networks, different activation functions, or different layer configurations.\n",
        " * Example: Adding more LSTM layers or using GRU instead of LSTM.\n",
        "2. Apply regularization techniques (e.g., Use dropout to prevent overfitting).\n",
        "3. Use advanced text preprocessing.\n",
        " * Implement techniques like stemming, lemmatization, or BPE (Byte Pair Encoding) for\n",
        "better tokenization.\n",
        "4. Fine-tune hyperparameters (e.g., learning rate, batch size).\n",
        " * Experiment with different learning rates, batch sizes, and epochs.\n",
        " * Explain your approach for fine-tuning the hyper-parameters."
      ],
      "metadata": {
        "id": "NAmXTKKUVXWc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q58FY70Yi4Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT fine tuned"
      ],
      "metadata": {
        "id": "nsGrcxrhJmec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation\n",
        "joined_text = \"\\n\".join(cleaned_dialogues)"
      ],
      "metadata": {
        "id": "kvz-GlRhJn_n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpt_2_simple"
      ],
      "metadata": {
        "id": "RwsWfKmmJswg",
        "outputId": "c0c2e445-e67a-4855-9f02-266a906c45d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpt_2_simple\n",
            "  Downloading gpt_2_simple-0.8.1.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from gpt_2_simple) (2.18.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from gpt_2_simple) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from gpt_2_simple) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gpt_2_simple) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from gpt_2_simple) (2.0.2)\n",
            "Collecting toposort (from gpt_2_simple)\n",
            "  Downloading toposort-1.10-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.5.1->gpt_2_simple) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.5.1->gpt_2_simple) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.5.1->gpt_2_simple) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.5.1->gpt_2_simple) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.5.1->gpt_2_simple) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.5.1->gpt_2_simple) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.5.1->gpt_2_simple) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.5.1->gpt_2_simple) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.5.1->gpt_2_simple) (5.29.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.5.1->gpt_2_simple) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.5.1->gpt_2_simple) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.5.1->gpt_2_simple) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.5.1->gpt_2_simple) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.5.1->gpt_2_simple) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.5.1->gpt_2_simple) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.5.1->gpt_2_simple) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.5.1->gpt_2_simple) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.5.1->gpt_2_simple) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.5.1->gpt_2_simple) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.5.1->gpt_2_simple) (0.37.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->gpt_2_simple) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->gpt_2_simple) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->gpt_2_simple) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->gpt_2_simple) (2025.4.26)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt_2_simple) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.5.1->gpt_2_simple) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.5.1->gpt_2_simple) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.5.1->gpt_2_simple) (0.15.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.5.1->gpt_2_simple) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.5.1->gpt_2_simple) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.5.1->gpt_2_simple) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow>=2.5.1->gpt_2_simple) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.5.1->gpt_2_simple) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.5.1->gpt_2_simple) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.5.1->gpt_2_simple) (0.1.2)\n",
            "Downloading toposort-1.10-py3-none-any.whl (8.5 kB)\n",
            "Building wheels for collected packages: gpt_2_simple\n",
            "  Building wheel for gpt_2_simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt_2_simple: filename=gpt_2_simple-0.8.1-py3-none-any.whl size=24557 sha256=88f53af682582d8676227f3be7e4fbabe76d334c641b63b5971ab4afc8180532\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/59/88/2abf9f043f52307bb3d81010e26ecdb5e539b392e8aca2501f\n",
            "Successfully built gpt_2_simple\n",
            "Installing collected packages: toposort, gpt_2_simple\n",
            "Successfully installed gpt_2_simple-0.8.1 toposort-1.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(joined_text)"
      ],
      "metadata": {
        "id": "tcBg_R4VJ2JW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gpt_2_simple as gpt2\n",
        "import tensorflow as tf\n",
        "gpt2.download_gpt2(model_name='124M')\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.finetune(sess,dataset='/content/output.txt'\n",
        "             ,model_name='124M'\n",
        "             ,steps=200\n",
        "             ,print_every=200\n",
        "             ,sample_every=100,\n",
        "              restore_from='fresh'\n",
        "\n",
        "             )"
      ],
      "metadata": {
        "id": "6AyvD2FaJvKI",
        "outputId": "e9c705ec-2fc3-4301-9160-13ce2e6c1f9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 3.84Git/s]                                                     \n",
            "Fetching encoder.json: 1.05Mit [00:02, 519kit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 5.10Git/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [02:56, 2.82Mit/s]\n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 3.27Git/s]                                               \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:01, 878kit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:01, 872kit/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:15<00:00, 15.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 4667336 tokens\n",
            "Training...\n",
            "======== SAMPLE 1 ========\n",
            " a you're doing a goddamn thing right now, he's doing a goddamn thing. that's a fucking thing. that's just something to fucking get a feel yourself. that's not good, and he's still good enough to do it like,\n",
            "\n",
            "your dad is a fucking dope. the only thing i don't like is he's the only goddamn thing fucking nice about it.\n",
            "\n",
            "i didn't watch any of that kid play baseball. i was just getting started.\n",
            "\n",
            "i guess it's just you, or it just gets worse and worse.\n",
            "\n",
            "not that you don't like it.\n",
            "\n",
            "yeah, but you'd be the better coach. there's just no way you'd ever get that out of me, anyway. i can tell you who's better, my dad.\n",
            "\n",
            "hey, look!\n",
            "oh, you know, he's great!\n",
            "yeah, but you're too goddamn smart.\n",
            "you wouldn't. you'd call yourself the smartest person in the room.\n",
            "it was nice. i could see what was taking place.\n",
            "well, it's gotta be nice.\n",
            "you, uh, i don't know anyone like that. you want me to meet those guys?\n",
            "you gotta be super smart, you fucking idiot. you're gonna be better later, right? look, i can't wait for them to see my dad.\n",
            "you want to do that? you want to work on me, man?\n",
            "you really want to do that?\n",
            "i want to know. can you talk to me? i mean, can you? you're going to tell him i'm doing anything?\n",
            "he's gonna say, 'goodnight, man' then, like my daddy used to say, 'hey, look at this!'\n",
            "i'm never gonna go out here. you're gonna say something stupid.\n",
            "it's just a game. the other day, i was playing with this guy, so he was like, 'yeah, the same guy i told you to meet the next day. 'but all he wants is a little something.' and we had two games where he was just going down, yelling at me. and i just stood there, not talking to him. and no one would know who was in that fight. it was just him and me.\n",
            "i'd been thinking about this thing for days.\n",
            "just a game. it's like you're playing poker in my house. all we're gonna do is say, 'what happened to my dad?' 'uhm', man, you're going to say that a lot. you're going to make me look like some freak.\n",
            "i always think i understand something about that kid.\n",
            "no.\n",
            "no, but it's gotta be good. it's gotta be good.\n",
            "and then it was like, 'look, you know what i'm talking about, but then you start telling me things you shouldn't even do.\n",
            "oh, man, you're so fucking smart. you really know what i'm talking about with that kid.\n",
            "i mean, if you'd just come back and put me on my own as a wrestler, my dad would be the best coach in the world.\n",
            "i mean, come on, you wanna be good for your friends?\n",
            "not because you wanna stay there with your mom.\n",
            "no. but because you wanna be good for the people you live with.\n",
            "who doesn't want that?\n",
            "oh, my god. the guy's just trying to get that old school kid on his side, and that's what the whole thing's gonna look like.\n",
            "he had a fight and a fight and he's gonna be all over you.\n",
            "yeah.\n",
            "it was. my dad.\n",
            "he was a good wrestler. he fought big.\n",
            "yeah. he's just kinda good.\n",
            "it's like the ol' thing.\n",
            "yeah. all you have to do is get into that fight and you can stay away.\n",
            "that was great.\n",
            "i don't know.\n",
            "it was a good fight. great.\n",
            "yeah. that was great.\n",
            "he's playing poker. that's great.\n",
            "he's playing poker.\n",
            "that's great.\n",
            "oh, my god. that's great.\n",
            "oh yeah. that was great.\n",
            "oh yeah.\n",
            "it was really great.\n",
            "uhm. uhh mm. that's great. that's great.\n",
            "the guy was good. then. it's like the first one's gonna knock you out the first time.\n",
            "oh. yeah?\n",
            "okay?\n",
            "oh, he was.\n",
            "oh, he was...\n",
            "yes. he's...\n",
            "oh uhh mm. yay!\n",
            "oh, yeah. he's a good wrestler.\n",
            "i'm not going to talk to you about that. i'm going to give you a call, and you could do something.\n",
            "what? how you feeling?\n",
            "oh, yeah. he's the guy.\n",
            "you're gonna like him\n",
            "\n"
          ]
        }
      ]
    }
  ]
}